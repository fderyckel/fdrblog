{
  "hash": "3b0dac58d23d75a0f257b9c3b8ba4503",
  "result": {
    "markdown": "---\ntitle: \"Kmeans with regime changes\"\nauthor: \"Francois de Ryckel\"\ndate: \"2022-10-12\"\ncategories: [kmeans, code, analysis, tidymodel]\neditor: source\n---\n\n\nThis post is about how to use Kmeans to classify various market regimes or to use kmeans to classifiy financial observations. \n\n::: {.callout-tip appearance=\"simple\"} \n\n# market regime \n\nFinancial markets have the tendency to change their behavior over time, which can create regimes, or periods of fairly persistent market conditions. Investors often look to discern the current market regime, looking out for any changes to it and how those might affect the individual components of their portfolio’s asset allocation. Modeling various market regimes can be an effective tool, as it can enable macroeconomically aware investment decision-making and better management of tail risks. \n\n:::\n\nWith K-means we are trying to establish groups of data that are **homegenous** and **distinctly different** from other groups.  The *K-* stands for the number of clusters we will create.  \n\nThe concept of distance comes in when deciding if a data point belongs to a cluster. The most common way to measure distance is the **Euclidean Distance**.  \n\nWith multivariate data set, it is important to normalize the data.  \nA usual rule of thumb is to set the number of clusters as the square root of the number of observation. \n\n# Using R \n\n## Load up packages and read data \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readr)        # load and read .csv file\nlibrary(glue)         # concatenate strings together\nlibrary(dplyr)        # the tidy plyr tool for data wrangling\nlibrary(tidyr)        # to use the drop_na function\n\nthe_path <- here::here()\n\ndf <- read_csv(glue(the_path, \"/raw_data/intc.csv\")) |> \n  rename(date = Date, high = High, low = Low, close = Close, adj_close = 'Adj Close') |> \n  select(date, high, low, close, adj_close)\nglimpse(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 10,738\nColumns: 5\n$ date      <dttm> 1980-03-17 05:00:00, 1980-03-18 05:00:00, 1980-03-19 05:00:…\n$ high      <dbl> 0.330729, 0.328125, 0.335938, 0.334635, 0.322917, 0.316406, …\n$ low       <dbl> 0.325521, 0.322917, 0.330729, 0.329427, 0.317708, 0.311198, …\n$ close     <dbl> 0.325521, 0.322917, 0.330729, 0.329427, 0.317708, 0.311198, …\n$ adj_close <dbl> 0.1907656, 0.1892397, 0.1938177, 0.1930547, 0.1861870, 0.182…\n```\n:::\n:::\n\n\n## Feature engineering \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(TTR)      # The technical analysis package\n\nyo <- aroon(df[, c('high', 'low')], n = 23)\ndf$aroon <- yo[, 3]\nyo <- CCI(df[, c('high', 'low', 'close')], n = 17)\ndf$cci <- yo\nyo <- chaikinVolatility(df[, c('high', 'low')], n = 13)\ndf$chaikinVol <- yo\n\ndf1 <- df |> \n  select(date, aroon, cci, chaikinVol, adj_close) |> \n  mutate(across(c(aroon, cci, chaikinVol), ~ as.numeric(scale(.)))) |>\n  drop_na()\n\nskimr::skim(df1 %>% select(-date))\n```\n\n::: {.cell-output-display}\nTable: Data summary\n\n|                         |                      |\n|:------------------------|:---------------------|\n|Name                     |df1 %>% select(-date) |\n|Number of rows           |10713                 |\n|Number of columns        |4                     |\n|_______________________  |                      |\n|Column type frequency:   |                      |\n|numeric                  |4                     |\n|________________________ |                      |\n|Group variables          |None                  |\n\n\n**Variable type: numeric**\n\n|skim_variable | n_missing| complete_rate|  mean|    sd|    p0|   p25|   p50|   p75|  p100|hist  |\n|:-------------|---------:|-------------:|-----:|-----:|-----:|-----:|-----:|-----:|-----:|:-----|\n|aroon         |         0|             1|  0.00|  1.00| -1.70| -0.95|  0.29|  0.91|  1.46|▆▆▂▇▇ |\n|cci           |         0|             1|  0.00|  1.00| -4.30| -0.80|  0.06|  0.78|  3.87|▁▃▇▅▁ |\n|chaikinVol    |         0|             1|  0.00|  1.00| -2.42| -0.62| -0.14|  0.42| 12.71|▇▂▁▁▁ |\n|adj_close     |         0|             1| 14.89| 15.23|  0.13|  0.77| 12.90| 19.88| 65.25|▇▅▁▁▁ |\n:::\n\n```{.r .cell-code}\n# also good to check for correlation between variables. \nlibrary(corrr)\ndf1 |> select(-date, -adj_close) |> \n  correlate() |> \n  rearrange() |> \n  shave()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 4\n  term            cci   aroon chaikinVol\n  <chr>         <dbl>   <dbl>      <dbl>\n1 cci        NA       NA              NA\n2 aroon       0.567   NA              NA\n3 chaikinVol -0.00427  0.0211         NA\n```\n:::\n:::\n\n\nThese 3 variables seem to complete each other well as little to-no correlation. \n\n## Create clusters\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(purrr)     #use the map function\nlibrary(broom)     #use the glance function on kmeans \ndf1sc <- df1 %>% select(-date, -adj_close)\n\nkclusts <- tibble(k = 1:9) |> \n  mutate(kclust = map(k, ~kmeans(df1sc, centers = .x, iter.max = 50L)), \n         glanced = map(kclust, glance), \n         augmented = map(kclust, augment, df1))\n\nkclusts |> unnest(cols = c('glanced'))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 9 × 7\n      k kclust    totss tot.withinss betweenss  iter augmented            \n  <int> <list>    <dbl>        <dbl>     <dbl> <int> <list>               \n1     1 <kmeans> 32127.       32127.  2.36e-10     1 <tibble [10,713 × 6]>\n2     2 <kmeans> 32127.       19719.  1.24e+ 4     1 <tibble [10,713 × 6]>\n3     3 <kmeans> 32127.       15887.  1.62e+ 4     5 <tibble [10,713 × 6]>\n4     4 <kmeans> 32127.       13538.  1.86e+ 4     4 <tibble [10,713 × 6]>\n5     5 <kmeans> 32127.       11165.  2.10e+ 4     5 <tibble [10,713 × 6]>\n6     6 <kmeans> 32127.        9921.  2.22e+ 4     4 <tibble [10,713 × 6]>\n7     7 <kmeans> 32127.        8508.  2.36e+ 4     5 <tibble [10,713 × 6]>\n8     8 <kmeans> 32127.        7978.  2.41e+ 4     6 <tibble [10,713 × 6]>\n9     9 <kmeans> 32127.        7250.  2.49e+ 4     6 <tibble [10,713 × 6]>\n```\n:::\n:::\n\n\nThere are several ways to choose the ideal number of clusters.  One of them is the elbow method, another one is the Silhouette Method. \n\nThe **tot.withinss** is the total within-cluster sum of square. This is the value used for the eblow method. \n\nFor the Silhouette Method, we can use the **cluster** package. \n\n\n::: {.cell hash='index_cache/html/calculate_silhoutte_value_in_r_288a26a38a6f37fd41d3cf3af8e7151e'}\n\n```{.r .cell-code}\navg_sil <- function(k) { \n  kmeans_object <- kmeans(df1sc, centers = k, iter.max = 50L)\n  silh = cluster::silhouette(kmeans_object$cluster, dist(df1sc))\n  mean(silh[, 3])\n  }\n\n# Compute and plot wss for k = 2 to k = 15\nyo <- tibble(k_values =  2:9) |> \n  mutate(avg_sil_values = map_dbl(k_values, avg_sil))\n\nyo\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 8 × 2\n  k_values avg_sil_values\n     <int>          <dbl>\n1        2          0.376\n2        3          0.369\n3        4          0.307\n4        5          0.312\n5        6          0.309\n6        7          0.307\n7        8          0.298\n8        9          0.280\n```\n:::\n:::\n\n\nA more elegant way to do that, using [this post from SO](https://stackoverflow.com/questions/63780363/r-using-purrr-map-function-to-calculate-silhouette-distances-of-kmeans-model)\n\n::: {.cell hash='index_cache/html/calculate_silhoutte_value_in_r_elegant_3840842e675ca5fb8c3d3b106271c693'}\n\n```{.r .cell-code}\nyo <- kclusts |> \n  mutate(silhouetted = map(augmented, ~ cluster::silhouette(as.numeric(levels(.x$.cluster))[.x$.cluster], dist(df1sc)))) |> \n  select(k, silhouetted) |> unnest(cols=c('silhouetted')) |> \n  group_by(k) %>% \n  summarise(avg_sil_values = mean(silhouetted[,3]))\n\nyo\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 9 × 2\n      k avg_sil_values\n  <int>          <dbl>\n1     1         NA    \n2     2          0.376\n3     3          0.369\n4     4          0.307\n5     5          0.292\n6     6          0.309\n7     7          0.307\n8     8          0.294\n9     9          0.280\n```\n:::\n:::\n\n\n\n\n## Some visualizations \n\n### Elbow method \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nkclusts |> \n  unnest(cols = c('glanced')) |> \n  ggplot(aes(k, tot.withinss)) + \n  geom_line(alpha = 0.5, size = 1.2, color = 'midnightblue') + \n  geom_point(size = 2, color = 'midnightblue')\n```\n\n::: {.cell-output-display}\n![Total within-cluster sum of square for k-cluster](index_files/figure-html/vizualise_kmeans_elbow-1.png){width=672}\n:::\n:::\n\nBased on the elbow method, I would be tempted to choose to 5 clusters (2 seems another obvious one).  \n\n### Silhouette Method \n\n\n::: {.cell}\n\n```{.r .cell-code}\nyo |> ggplot(aes(k, avg_sil_values)) + \n  geom_line(alpha = 0.5, size = 1.2, color = 'midnightblue') + \n  geom_point(size = 2, color = 'midnightblue')\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 1 row(s) containing missing values (geom_path).\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 1 rows containing missing values (geom_point).\n```\n:::\n\n::: {.cell-output-display}\n![Silhouette score for k-clusters](index_files/figure-html/silhouette_score_graph_r-1.png){width=672}\n:::\n:::\n\n2 is the winner ;-) \n\n### Plotting the stocks with clustered observations \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lubridate)\nyo <- kmeans(df1 |> select(-date, -adj_close), centers = 2)\naugment(yo, df1) |> filter(date >= today() - 500) |> \n  ggplot(aes(x = date, y = adj_close)) + \n    geom_line(alpha = 0.5, color = 'midnightblue') + \n    geom_point(aes(color = .cluster)) + \n    theme(legend.position = 'none')\n```\n\n::: {.cell-output-display}\n![Plotting adjusted close price with only 2 clusters](index_files/figure-html/plotting_with_2clusters_observation-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nyo <- kmeans(df1 |> select(-date, -adj_close), centers = 3)\naugment(yo, df1) |> filter(date >= today() - 500) |> \n  ggplot(aes(x = date, y = adj_close)) + \n    geom_line(alpha = 0.5, color = 'midnightblue') + \n    geom_point(aes(color = .cluster)) + \n    theme(legend.position = 'none')\n```\n\n::: {.cell-output-display}\n![Plotting adjusted close price with only 3 clusters](index_files/figure-html/plotting_with_only_3clusters-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nyo <- kmeans(df1 |> select(-date, -adj_close), centers = 6)\naugment(yo, df1) |> filter(date >= today() - 500) |> \n  ggplot(aes(x = date, y = adj_close)) + \n    geom_line(alpha = 0.5, color = 'midnightblue') + \n    geom_point(aes(color = .cluster)) + \n    theme(legend.position = 'none')\n```\n\n::: {.cell-output-display}\n![Plotting adjusted close price with only 6 clusters](index_files/figure-html/platting_with_only_6clusters-1.png){width=672}\n:::\n:::\n\n\n\n# Using python \nOriginal blog post \n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport yfinance as yf     #only to download data\ndata  = yf.download(\"INTC\")\ndata.to_csv(\"../../raw_data/intc.csv\")\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\n\npy_df = pd.read_csv(\"../../raw_data/intc.csv\", names = ['date', 'open', 'high', 'low', 'close', 'adj_close', 'volume']).iloc[1: , :]\npy_df.tail()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                            date  ...    volume\n10734  2022-10-10 00:00:00-04:00  ...  43409300\n10735  2022-10-11 00:00:00-04:00  ...  48134900\n10736  2022-10-12 00:00:00-04:00  ...  39634100\n10737  2022-10-13 00:00:00-04:00  ...  62447000\n10738  2022-10-14 00:00:00-04:00  ...  48162500\n\n[5 rows x 7 columns]\n```\n:::\n\n```{.python .cell-code}\npy_df.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 10738 entries, 1 to 10738\nData columns (total 7 columns):\n #   Column     Non-Null Count  Dtype \n---  ------     --------------  ----- \n 0   date       10738 non-null  object\n 1   open       10738 non-null  object\n 2   high       10738 non-null  object\n 3   low        10738 non-null  object\n 4   close      10738 non-null  object\n 5   adj_close  10738 non-null  object\n 6   volume     10738 non-null  object\ndtypes: object(7)\nmemory usage: 587.4+ KB\n```\n:::\n\n```{.python .cell-code}\npy_df.shape\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(10738, 7)\n```\n:::\n\n```{.python .cell-code}\npy_df_melt = py_df.melt(id_vars = 'date', value_vars = ['open', 'high', 'low', 'close'], value_name = 'prices', var_name = 'price_point')\npy_df_melt.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 42952 entries, 0 to 42951\nData columns (total 3 columns):\n #   Column       Non-Null Count  Dtype \n---  ------       --------------  ----- \n 0   date         42952 non-null  object\n 1   price_point  42952 non-null  object\n 2   prices       42952 non-null  object\ndtypes: object(3)\nmemory usage: 1006.8+ KB\n```\n:::\n\n```{.python .cell-code}\npy_df_melt.shape\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(42952, 3)\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nimport talib as ta\n\ncci = ta.CCI(py_df.high, py_df.low, py_df.close, timeperiod = 17)\nht = ta.HT_DCPHASE(py_df.close)\naaron = ta.AROONOSC(py_df.high, py_df.low, timeperiod = 23)\n\nta_df = pd.DataFrame({\"date_time\": py_df['date'].to_list(), \"cci\": cci, \"aaron\":aaron, \"ht\": ht})\nta_df = ta_df.dropna()\npy_df = py_df.loc[ta_df.index, :]   #slicing to get same rows as those in ta_df\n\n#ta_df.pop(\"date_time\")\n```\n:::\n\n\nLet's graph the last year of data \n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport matplotlib.pyplot as plt\n\nta_df2 = ta_df.tail(250).copy()\nta_df2['adj_close'] = py_df['adj_close']\nta_df2['date_time'] = pd.to_datetime(ta_df2['date_time'], utc=True)\nta_df2['adj_close'] = pd.to_numeric(ta_df2['adj_close'])\n\nfig = plt.figure(figsize = (12, 8)) \ngs = fig.add_gridspec(3, hspace=0)\naxs = gs.subplots(sharex=True)\n#plt.figure(figsize = (12, 8))\naxs[0].plot(ta_df2['date_time'], ta_df2['adj_close'])\naxs[0].set_ylim(25, 55)\n#axs[0].set_title('INTC price')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(25.0, 55.0)\n```\n:::\n\n```{.python .cell-code}\naxs[1].plot(ta_df2['date_time'], ta_df2['aaron'],  'tab:green')\naxs[1].set_ylim(-105, 105)\n#axs[1].set_title('Aaron ind.')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(-105.0, 105.0)\n```\n:::\n\n```{.python .cell-code}\naxs[2].plot(ta_df2['date_time'], ta_df2['ht'], 'tab:red')\naxs[2].set_ylim(-50,320)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(-50.0, 320.0)\n```\n:::\n\n```{.python .cell-code}\nfor ax in axs:\n    ax.label_outer()\n    \nplt.show()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/graph_stocks_using_pyplot-1.png){width=1152}\n:::\n:::\n\n::: {.cell hash='index_cache/html/model_cluster_in_python_f09b3c31a3b3e24f3ebe9f06ed5cb43d'}\n\n```{.python .cell-code}\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.cluster import KMeans\n\ninertia = []\nsil_score = []\n\nta_df.pop('date_time')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n64       1980-06-16 00:00:00-04:00\n65       1980-06-17 00:00:00-04:00\n66       1980-06-18 00:00:00-04:00\n67       1980-06-19 00:00:00-04:00\n68       1980-06-20 00:00:00-04:00\n                   ...            \n10732    2022-10-06 00:00:00-04:00\n10733    2022-10-07 00:00:00-04:00\n10734    2022-10-10 00:00:00-04:00\n10735    2022-10-11 00:00:00-04:00\n10736    2022-10-12 00:00:00-04:00\nName: date_time, Length: 10673, dtype: object\n```\n:::\n\n```{.python .cell-code}\nfor n_clusters in range(2, 14): \n  kmeans = KMeans(n_clusters = n_clusters, random_state=0)\n  preds = kmeans.fit_predict(ta_df)\n  inertia.append(kmeans.inertia_ / n_clusters)\n  sil_score.append(silhouette_score(ta_df, preds))\n  \ninertias = pd.DataFrame({n_clusters: range(2, 14), \"inertia\": inertia})\nsil_scores = pd.DataFrame({n_clusters: range(2, 14), \"sil_score\": sil_score})\n\nprint(inertias)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    13       inertia\n0    2  7.618664e+07\n1    3  2.920819e+07\n2    4  1.718740e+07\n3    5  1.117580e+07\n4    6  8.267593e+06\n5    7  6.284493e+06\n6    8  5.081891e+06\n7    9  4.198391e+06\n8   10  3.538741e+06\n9   11  3.034879e+06\n10  12  2.630944e+06\n11  13  2.295893e+06\n```\n:::\n\n```{.python .cell-code}\nprint(sil_scores)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    13  sil_score\n0    2   0.443054\n1    3   0.472803\n2    4   0.429613\n3    5   0.372368\n4    6   0.332180\n5    7   0.309072\n6    8   0.299460\n7    9   0.281781\n8   10   0.286319\n9   11   0.280894\n10  12   0.277926\n11  13   0.277915\n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}