---
title: "Extrapolate with Decision Trees"
author: "Francois de Ryckel"
date: "2022-09-25"
categories: [decision_tree, code, analysis]
---

# Introduction 

The idea of this document is to show how Xgboost can be especially bad at extrapolating or make prediction out of range of given data.   This idea of this post come from [this blog](https://www.sarem-seitz.com/forecasting-with-decision-trees-and-random-forests/)

# Extrapolate linear trends 


## Using R 

```{r} 
#| message: false
#| warning: false
#| fig-width: 10
#| fig-height: 7
#| label: fig-r-lineartrend
#| fig-cap: "Performance of decision tree on a linear trend"

library(tibble)
library(dplyr)
library(ggplot2)
library(parsnip)

set.seed(123)
df <- tibble(t = seq(1:100), 
             y = t + 2 * rnorm(100, mean = 0, sd = 1))

#The first 50 observation to be used for training and the last 50 for testing.
df_train <- df[1:50, ]
df_test <- df[51:100, ]

model = decision_tree(mode = "regression", engine = "rpart", tree_depth = 2) 
model_fit = model %>% fit(y ~ t, data = df_train)
model_fit

y_predict <- add_row(predict(model_fit, df_train), predict(model_fit, df_test))

df <- df |> add_column( y_predict)
tail(df)

ggplot() + 
  geom_line(aes(x = t, y = y, color = "a"), data = df[1:50, ], show.legend = TRUE) + 
  geom_line(aes(x = t, y = y, color="a"), lty=2, data = df[50:100, ]) + 
  geom_line(aes(x=t, y = .pred, color = "b"), data = df[1:50, ]) + 
  geom_line(aes(x = t, y = .pred, color = "c"), lty=2, data = df[50:100, ]) + 
  scale_color_manual(name = "Legend", 
                     values = c("a" = "blue", "b" = "red", "c" = "purple"), 
                     labels = c("Data", "In sample testing", "Out of sample testing")) + 
  theme(legend.position = "bottom")
```


# Extrapolate using seasonal trend 

We could use a seasonal pattern and get the same results.

```{r}

(1 + 0.06/2)^10

```

The `echo: false` option disables the printing of code (only output is displayed).
