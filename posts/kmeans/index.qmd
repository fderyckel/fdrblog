---
title: "Kmeans with regime changes"
author: "Francois de Ryckel"
date: "2022-10-12"
categories: [kmeans, code, analysis, tidymodel]
editor: source
---

This post is about how to use Kmeans to classify various market regimes or to use kmeans to classifiy financial observations. 

With K-means we are trying to establish groups of data that are **homegenous** and **distinctly different** from other groups.  The *K-* stands for the number of clusters we will create.  

The concept of distance comes in when deciding if a data point belongs to a cluster. The most common way to measure distance is the **Euclidean Distance**.  

With multivariate data set, it is important to normalize the data.  
A usual rule of thumb is to set the number of clusters as the square root of the number of observation. 

# Using R 

## Loading up pckages and reading data 

```{r}
#| label: setting_up
#| warning: false
#| message: false

library(readr)        # load and read .csv file
library(glue)         # concatenate strings together
library(dplyr)        # the tidy plyr tool for data wrangling
library(tidyr)        # to use theh drop_na function

the_path <- here::here()

df <- read_csv(glue(the_path, "/raw_data/intc.csv")) |> 
  rename(date = Date, high = High, low = Low, close = Close, adj_close = 'Adj Close') |> 
  select(date, high, low, close, adj_close)
glimpse(df)
```

## feature engineering 

```{r}
#| label: features_engineering_in_r
#| warning: false
#| message: false

library(TTR)      # The technical analysis package

yo <- aroon(df[, c('high', 'low')], n = 23)
df$aroon <- yo[, 3]
yo <- CCI(df[, c('high', 'low', 'close')], n = 17)
df$cci <- yo
yo <- chaikinVolatility(df[, c('high', 'low')], n = 13)
df$chaikinVol <- yo

df1 <- df |> 
  select(date, aroon, cci, chaikinVol, adj_close) |> 
  mutate(across(c(aroon, cci, chaikinVol), ~ as.numeric(scale(.)))) |>
  drop_na()

skimr::skim(df1 %>% select(-date))

# also good to check for correlation between variables. 
library(corrr)
df1 |> select(-date, -adj_close) |> 
  correlate() |> 
  rearrange() |> 
  shave()
```

## Creating our kmeans clusters

```{r}
#| label: creating_cluster_in_r

library(purrr)     #use the map function
library(broom)     #use the glance function on kmeans 

kclusts <- tibble(k = 1:9) |> 
  mutate(kclust = map(k, ~kmeans(df1 %>% select(-date, -adj_close), centers = .x, iter.max = 50L)), 
         glanced = map(kclust, glance))

kclusts |> unnest(cols = c('glanced'))

```

There are several ways to choose the ideal number of clusters.  One of them is the elbow method, another one is the Silhouette Method. 

The **tot.withinss** is the total within-cluster sum of square. This is the value used for the eblow method. 

Let's try with the Silouhette Method.  We can use the **cluster** package for that. 

```{r}
#| label: calculate_silhoutte_value_in_r
#| cache: true

avg_sil <- function(k) { 
  kmeans_object <- kmeans(df1 |> select(-date, -adj_close), centers = k, iter.max = 50L)
  silh = cluster::silhouette(kmeans_object$cluster, dist(df1 |> select(-date, -adj_close)))
  mean(silh[, 3])
  }

# Compute and plot wss for k = 2 to k = 15
yo <- tibble(k_values =  2:9) |> 
  mutate(avg_sil_values = map_dbl(k_values, avg_sil))
```



## Some visualizations 

### Elbow method 

```{r}
#| label: vizualise_kmeans_elbow
#| fig-cap: "Total within-cluster sum of square for k-cluster"

library(ggplot2)
kclusts |> 
  unnest(cols = c('glanced')) |> 
  ggplot(aes(k, tot.withinss)) + 
  geom_line(alpha = 0.5, size = 1.2, color = 'midnightblue') + 
  geom_point(size = 2, color = 'midnightblue')
  
```
Based on the elbow method, I would be tempted to choose to 5 clusters (2 seems another obvious one).  

### Silhouette Method 

```{r}
#| label: silhouette_score_graph_r
#| fig-cap: "Silhouette score for k-clusters"

yo |> ggplot(aes(k_values, avg_sil_values)) + 
  geom_line(alpha = 0.5, size = 1.2, color = 'midnightblue') + 
  geom_point(size = 2, color = 'midnightblue')
  
```
2 is the winner ;-) 

### Plotting the stocks with clustered observations 

```{r}
library(lubridate)
yo <- kmeans(df1 |> select(-date, -adj_close), centers = 2)
augment(yo, df1) |> filter(date >= today() - 500) |> 
  ggplot(aes(x = date, y = adj_close)) + 
    geom_line(alpha = 0.5, color = 'midnightblue') + 
    geom_point(aes(color = .cluster)) + 
    theme(legend.position = 'none')

yo <- kmeans(df1 |> select(-date, -adj_close), centers = 5)
augment(yo, df1) |> filter(date >= today() - 500) |> 
  ggplot(aes(x = date, y = adj_close)) + 
    geom_line(alpha = 0.5, color = 'midnightblue') + 
    geom_point(aes(color = .cluster)) + 
    theme(legend.position = 'none')
```

# Using python 
Original blog post 

```{python}
#| label: get_and_save_the_data
#| eval: false

import yfinance as yf     #only to download data
data  = yf.download("INTC")
data.to_csv("../../raw_data/intc.csv")
```


```{python}
#| label: load_and_transform_data

import pandas as pd

py_df = pd.read_csv("../../raw_data/intc.csv", names = ['date', 'open', 'high', 'low', 'close', 'adj_close', 'volume']).iloc[1: , :]
py_df.tail()
py_df.info()
py_df.shape

py_df_melt = py_df.melt(id_vars = 'date', value_vars = ['open', 'high', 'low', 'close'], value_name = 'prices', var_name = 'price_point')
py_df_melt.info()
py_df_melt.shape

```

```{python}
#| label: features_engineering_with_python
#| warning: false
#| message: false

import talib as ta

cci = ta.CCI(py_df.high, py_df.low, py_df.close, timeperiod = 17)
ht = ta.HT_DCPHASE(py_df.close)
aaron = ta.AROONOSC(py_df.high, py_df.low, timeperiod = 23)

ta_df = pd.DataFrame({"date_time": py_df['date'].to_list(), "cci": cci, "aaron":aaron, "ht": ht})
ta_df = ta_df.dropna()
py_df = py_df.loc[ta_df.index, :]   #slicing to get same rows as those in ta_df

#ta_df.pop("date_time")
```

Let's graph the last year of data 

```{python}
#| label: graph_stocks_using_pyplot
#| warning: false
#| message: false

import matplotlib.pyplot as plt

ta_df2 = ta_df.tail(250).copy()
ta_df2['adj_close'] = py_df['adj_close']
ta_df2['date_time'] = pd.to_datetime(ta_df2['date_time'], utc=True)
ta_df2['adj_close'] = pd.to_numeric(ta_df2['adj_close'])

fig = plt.figure(figsize = (12, 8)) 
gs = fig.add_gridspec(3, hspace=0)
axs = gs.subplots(sharex=True)
#plt.figure(figsize = (12, 8))
axs[0].plot(ta_df2['date_time'], ta_df2['adj_close'])
axs[0].set_ylim(25, 55)
#axs[0].set_title('INTC price')
axs[1].plot(ta_df2['date_time'], ta_df2['aaron'],  'tab:green')
axs[1].set_ylim(-105, 105)
#axs[1].set_title('Aaron ind.')
axs[2].plot(ta_df2['date_time'], ta_df2['ht'], 'tab:red')
axs[2].set_ylim(-50,320)

for ax in axs:
    ax.label_outer()
    
plt.show()
```


```{python}
#| label: model_cluster_in_python
#| cache: true

from sklearn.metrics import silhouette_score
from sklearn.cluster import KMeans

inertia = []
sil_score = []

ta_df.pop('date_time')

for n_clusters in range(2, 14): 
  kmeans = KMeans(n_clusters = n_clusters, random_state=0)
  preds = kmeans.fit_predict(ta_df)
  inertia.append(kmeans.inertia_ / n_clusters)
  sil_score.append(silhouette_score(ta_df, preds))
  
inertias = pd.DataFrame({n_clusters: range(2, 14), "inertia": inertia})
sil_scores = pd.DataFrame({n_clusters: range(2, 14), "sil_score": sil_score})

print(inertias)
print(sil_scores)
```

